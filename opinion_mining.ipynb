{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d62ae59",
   "metadata": {},
   "source": [
    "# Objectifs: construire différents systèmes de classification d'opinion.\n",
    "- essayer et évaluer différentes représentations de documents\n",
    "- évaluer différents classifieur\n",
    "- ajustage des représentations et classifieurs\n",
    "- obtenir les meilleurs résultats possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8d835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac93e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#pour l'évaluation des classifieurs\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73529a",
   "metadata": {},
   "source": [
    "## Chargement des données d'apprentissage et de test\n",
    "Faire une fonction qui lit les données et renvoie X et Y:\n",
    "- RawXtrain = liste tweets (tweets comme chaines de caractères)\n",
    "- RawYtrain = liste d'étiquettes d'opinion\n",
    "\n",
    "idem pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ede55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eaf7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    f = open(file,\"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        X.append(line)\n",
    "        print(line, end=\"\")\n",
    "    X.append()\n",
    "    return X, Y\n",
    "\n",
    "RawXtrain, RawYtrain = load(\"OpinionMining.data\")\n",
    "RawXtest, RawYtest = load(\"OpinionMining.test\")\n",
    "print(RawXtrain[0:3])\n",
    "print(RawYtrain[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba40fcb",
   "metadata": {},
   "source": [
    "## Représentation des tweets avec [Spacy](https://spacy.io/usage/processing-pipelines) (vecteur moyen des mots du tweet)\n",
    "Faire une fonction qui prends les données brutes et renvoie les données prêtes à classifier (X,Y):\n",
    "- X est l'entrée du classifieur: matrice(nbtweets,vectorsize)\n",
    "- Y est la sortie attendu du classifieur: vecteur d'étiquettes (numéro unique dans un dictionnaire [0,nbclasses[)\n",
    "\n",
    "Vous aurez également besoin de:\n",
    "- une fonction pour générer le dictionnaire classe -> numéro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e192086",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6d/4gm1cz3d5rj4l39yqmm8r7h40000gn/T/ipykernel_1393/3430066117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fr_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#télécharger le modèle avant: python3 -m spacy download fr_core_news_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "#télécharger le modèle avant: python3 -m spacy download fr_core_news_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ca07b",
   "metadata": {},
   "source": [
    "## Classification et évaluation\n",
    "- produisez les données d'entrée du classifieur pour l'apprentissage et le test avec la fonction de représentation spacy précédemment écrite\n",
    "- écrivez une fonction qui prend en paramètre les données et fait la classification et l'évaluation avec [scikit-learn](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeding (X_brute):\n",
    "    X[]\n",
    "    docs = nlp.pipe(X) #créer tout le vocabulaire (vectorSize)\n",
    "    for doc in docs:\n",
    "        #doc.vector  #moyenne des vecteurs d'emmbedings des mots du documents \n",
    "        for tokens in doc:\n",
    "            X.append(tokens.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(Xtrain,Ytrain,Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca793d9",
   "metadata": {},
   "source": [
    "## Représentation de document avec modèle vectoriel tf.idf\n",
    "- écrire une fonction qui représente les entrées brutes (RawXtrain et RawXtest) avec un [modèle vectoriel type sac-de-mot](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "- appelez votre fonction de classification précédemment écrite avec les nouvelles données et comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ede2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "classify(Xtrain,Ytrain,Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0fab1",
   "metadata": {},
   "source": [
    "## Réseaux de neurones récurrent\n",
    "\n",
    "Ces réseaux de neurones récurrent peuvent analyser chaque mot de la phrase, les représenter en interne puis les classer. Pour des raisons de facilité, les tweets vont être stockés dans une matrice carré peu importe le nombre de mots dans les tweets: on basera la taille de la matrice en fonction du tweet le plus long et nous ferons du bourrage pour les tweets plus court. Pratiquemment:\n",
    "\n",
    "- créez un dictionnaire de mots (similaires au dictionnaire de classes) sur l'apprentissage (chaque mot unique de la phrase que vous décidez de conserver aura un numéro unique), avec 2 entrées à part: une entrée 0 pour le bourrage, et une entrée 1 pour les mots inconnus que nous trouverons dans le test\n",
    "- créez une fonction qui prends le corpus brut en entrée et le dictionnaire et qui renvoi l'entrée attendu du RNN: une matrice de taille (nbtweets*55) ou 55 est le nombre de mots du tweet le plus long, cette matrice code à chaque ligne un tweet, avec sur la ligne les mots du tweet (leur numéro dans le dictionnaire), pour les tweets plus court que 55, on bourre à droite de 0\n",
    "- puis faites une architecture de réseau récurrent au moyen de [tensorflow.keras](https://www.tensorflow.org/guide/keras/rnn) pour cette tâche\n",
    "- entrainez, évaluez, comparez le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#générer dictionnaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crée une fonction qui met en forme les données\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#écrire fonction RNN qui crée le classifieur l'entraine et l'évalue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e148106",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN(Xtrain,Ytrain,Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2aaa6",
   "metadata": {},
   "source": [
    "## Vous avez les bases pour tenter d'améliorer la chaine de traitement\n",
    "\n",
    "- prétraiter les données: ne pas garder tous les mots, généraliser certains token (url ?)\n",
    "- ajustez bien vos classifieurs: avec [scikit-learn](https://scikit-learn.org/stable/model_selection.html#model-selection)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5eb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
